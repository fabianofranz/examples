{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af99f876-0ffd-4079-aeb7-4cead05daaf4",
   "metadata": {},
   "source": [
    "# üê∂ Data Pre-Processing: From source PDF to SDG-ready\n",
    "\n",
    "This notebook goes through each of the stages of data pre-processing. Directory-based conventions are used to save intermediate results as a PDF is converted and chunked, QA generation is performed to create a `qna.yaml` file, and finally everything is combined into the inputs for SDG.\n",
    "\n",
    "Once a SDG seed dataset is created, a user can run through an SDG notebook and generate samples.\n",
    "\n",
    "**NOTE**: Starting the notebook using Python 3.11 is recommended. Python 3.12 or later are not yet supported. \n",
    "\n",
    "1. [Document Conversion](#Document-Conversion)\n",
    "1. [Chunking](#Chunking)\n",
    "1. [Authoring](#Authoring)\n",
    "1. [Create Seed Dataset](#Create-Seed-Dataset-for-SDG)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0acd026f-65bd-4393-bb40-f8aa8bd6828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORKSPACE_NAME = \"default\"\n",
    "\n",
    "WORKSPACE_ROOT = Path(\"workspaces\")\n",
    "WORKSPACE_ROOT.mkdir(exist_ok=True)\n",
    "\n",
    "WORKSPACE_DIR = WORKSPACE_ROOT / WORKSPACE_NAME\n",
    "WORKSPACE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SOURCE_DOCUMENT = '/home/ffranz/Dev/e2e-poc-source-documents/safebalance-clarity-statement.pdf' # to process a specific document, set its path here; otherwise, the entire source documents repository will be used\n",
    "SOURCE_DOCUMENT_DIR = WORKSPACE_DIR / \"source_documents\"\n",
    "SOURCE_DOCUMENT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONVERSION_OUTPUT_DIR = WORKSPACE_DIR / \"conversion\"\n",
    "CONVERSION_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CHUNKING_OUTPUT_DIR = WORKSPACE_DIR / \"chunking\"\n",
    "CHUNKING_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "AUTHORING_OUTPUT_DIR = WORKSPACE_DIR / \"authoring\"\n",
    "AUTHORING_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SEED_EXAMPLE_INPUT_DIR = WORKSPACE_DIR / \"sdg_inputs\"\n",
    "SEED_EXAMPLE_INPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SEED_EXAMPLE_OUTPUT_DIR = WORKSPACE_DIR / \"seed_examples\"\n",
    "SEED_EXAMPLE_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "SDG_OUTPUT_DIR = WORKSPACE_DIR / \"sdg\"\n",
    "SDG_OUTPUT_DIR.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344b7ac5-fc2a-40a8-8e1f-e8dd8b1153e7",
   "metadata": {},
   "source": [
    "## Document Conversion\n",
    "\n",
    "This notebook uses [Docling](https://github.com/docling-project/docling) to convert any type of document into a Docling Document. A Docling Document is the representation of the document after conversion that can be exported as JSON. The JSON output of this notebook can then be used in others such as one that uses Docling's chunking methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91d4b2e-19cd-46e7-a912-ba9b2904c7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq docling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3804ef-4961-44b1-91c9-62929f422702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files to convert: [PosixPath('/home/ffranz/Dev/e2e-poc-source-documents/safebalance-clarity-statement.pdf')]\n"
     ]
    }
   ],
   "source": [
    "files = []\n",
    "\n",
    "if SOURCE_DOCUMENT:\n",
    "    files.append(Path(SOURCE_DOCUMENT))\n",
    "else:\n",
    "    print(\"***** WARNING! Only one file at a time is supported at this time.\")\n",
    "    files = list(SOURCE_DOCUMENT_DIR.rglob(\"*.pdf\"))\n",
    "    print(f\"***** Using {files[0]})\")\n",
    "\n",
    "print(f\"Files to convert: {files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749fb64b-d089-4844-9330-7f3639819e7a",
   "metadata": {},
   "source": [
    "### Configure Docling conversion pipeline\n",
    "\n",
    "Next we set the configuration options for our conversion pipeline. The PDF Conversion options set here are the defaults. More information about pipeline configuration can be found on Docling.\n",
    "\n",
    "For a complete reference on Docling conversion pipeline configuration, see [PDFPipelineOptions](https://docling-project.github.io/docling/reference/pipeline_options/#docling.datamodel.pipeline_options.PdfPipelineOptions) and [PDFFormatOptions](https://docling-project.github.io/docling/reference/document_converter/#docling.document_converter.InputFormat.XML_JATS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "157c5e02-edd1-44f6-b20f-f6b4bda1aae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffranz/Dev/venv/lib64/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "\n",
    "pipeline_options = PdfPipelineOptions() # TODO: show the options that can be set\n",
    "\n",
    "doc_converter = DocumentConverter(\n",
    "    format_options={\n",
    "        InputFormat.PDF: PdfFormatOption(\n",
    "            pipeline_options=pipeline_options\n",
    "        )\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73400c74-dead-4998-aee2-ddb00ddaa276",
   "metadata": {},
   "source": [
    "Finally, we convert every document into Docling JSON as long as it is a valid file type to be converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a200039c-b8b2-4087-88ba-7bfb0e393cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffranz/Dev/venv/lib64/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/ffranz/Dev/venv/lib64/python3.11/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path of JSON output is: /home/ffranz/Dev/examples/notebooks/instructlab-knowledge/workspaces/default/conversion/safebalance-clarity-statement.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffranz/Dev/venv/lib64/python3.11/site-packages/docling/pipeline/standard_pdf_pipeline.py:262: RuntimeWarning: Mean of empty slice\n",
      "  np.nanmean(\n",
      "/home/ffranz/Dev/venv/lib64/python3.11/site-packages/docling/pipeline/standard_pdf_pipeline.py:267: RuntimeWarning: Mean of empty slice\n",
      "  np.nanmean(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "for file in files:\n",
    "    doc = doc_converter.convert(source=file).document\n",
    "    doc_dict = doc.export_to_dict()\n",
    "\n",
    "    json_output_path = CONVERSION_OUTPUT_DIR / f\"{file.stem}.json\"\n",
    "    with open(json_output_path, \"w\") as f:\n",
    "        json.dump(doc_dict, f)\n",
    "        print(f\"Path of JSON output is: {Path(json_output_path).resolve()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafad55e-a4c0-4d6e-9da0-49519fa9bf74",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "The goal of chunking the converted documents is to provide the teacher model small and logical pieces of the source document to generate data off of.\n",
    "\n",
    "In this notebook we are doing chunking with [Docling](https://docling-project.github.io/docling/examples/hybrid_chunking/#hybrid-chunking).\n",
    "\n",
    "The input to this notebook is a docling JSON file created after a docling conversion, or a directory of docling JSON files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2482060c-a49f-4345-aa47-d54301939387",
   "metadata": {},
   "source": [
    "### Initialize the Chunker\n",
    "\n",
    "Docling provides two chunkers, the `HierarchicalChunker` and the `HybridChunker`.\n",
    "The `HierarchicalChunker` creates chunks based on the hierarchy in the Docling document\n",
    "\n",
    "The `HybridChunker` builds on the `HierarchicalChunker` and by making it tokenization aware.\n",
    "\n",
    "The `HybridChunker` has options for a `tokenizer`, the `max_tokens` in a chunk, and whether to merge undersized peer chunks. Uncomment the commented out code to configure these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50df9d91-add4-46a1-a69d-0f7f9f69542e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from docling_core.transforms.chunker.tokenizer.huggingface import HuggingFaceTokenizer\n",
    "#from transformers import AutoTokenizer\n",
    "\n",
    "from docling.chunking import HybridChunker\n",
    "\n",
    "#EMBED_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "#MAX_TOKENS = 1024\n",
    "#\n",
    "# tokenizer = HuggingFaceTokenizer(\n",
    "#     tokenizer=AutoTokenizer.from_pretrained(EMBED_MODEL_ID),\n",
    "#     max_tokens=MAX_TOKENS,  # optional, by default derived from `tokenizer` for HF case\n",
    "#     merge_peers=True # \n",
    "# )\n",
    "\n",
    "chunker = HybridChunker(\n",
    "    #tokenizer=tokenizer,\n",
    "    #merge_peers=True,  # whether to merge undersized chunks - defaults to True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ce1d6f-b8d3-470c-b3c9-675911f0ee92",
   "metadata": {},
   "source": [
    "### Load and chunk the converted docling document\n",
    "\n",
    "Next lets convert the document we want to chunk up into a Docling Document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db983c05-4aa6-4261-9283-2adab69bfbd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffranz/Dev/venv/lib64/python3.11/site-packages/numpy/_core/fromnumeric.py:3860: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/ffranz/Dev/venv/lib64/python3.11/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 12 chunks from safebalance-clarity-statement\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ffranz/Dev/venv/lib64/python3.11/site-packages/docling/pipeline/standard_pdf_pipeline.py:262: RuntimeWarning: Mean of empty slice\n",
      "  np.nanmean(\n",
      "/home/ffranz/Dev/venv/lib64/python3.11/site-packages/docling/pipeline/standard_pdf_pipeline.py:267: RuntimeWarning: Mean of empty slice\n",
      "  np.nanmean(\n"
     ]
    }
   ],
   "source": [
    "all_chunks = []\n",
    "docs = []\n",
    "for file in files:\n",
    "    doc = DocumentConverter().convert(source=file)\n",
    "    docs.append(doc)\n",
    "    \n",
    "    chunk_iter = chunker.chunk(dl_doc=doc.document)\n",
    "    chunk_objs = list(chunk_iter)\n",
    "    chunks = [chunker.contextualize(chunk=chunk) for chunk in chunk_objs]\n",
    "\n",
    "    print(f\"Extracted {len(chunks)} chunks from {doc.document.name}\")\n",
    "    \n",
    "    for chunk in chunks:\n",
    "        c = dict(chunk=chunk, file=file.stem)\n",
    "        all_chunks.append(c)\n",
    "\n",
    "# TODO: support multiple files save all chunks to single file for review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb38545-eb84-4923-8fc4-d10ed08eab26",
   "metadata": {},
   "source": [
    "### View the Chunks\n",
    "\n",
    "To view the chunks, run through the following cell. As you can see the document is broken into small pieces with metadata about the chunk based on the document's format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fdf34c7-9829-43d2-bf9f-7d1d55bb6a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional fees\n",
      "Per transaction, greater of $5.00 OR 3% of the amount (maximum $10.00) when you use your ATM or debit card, or card number, to make a withdrawal, transfer or payment at another bank and it is processed as a cash disbursement.\n"
     ]
    }
   ],
   "source": [
    "#print(all_chunks)\n",
    "print(chunks[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c4160f-7508-4c72-b28d-b56aa4975b26",
   "metadata": {},
   "source": [
    "### Save the chunks to a text file for each chunk\n",
    "\n",
    "Each chunk is saved to an individual text file in the format: `{docling-json-file-name}-{chunk #}.txt`. Having chunking in this format is important as an input to create-sdg-seed-data notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e70d576-a2bc-4274-b660-1cbe051968b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(all_chunks):\n",
    "    chunk_path = CHUNKING_OUTPUT_DIR / f\"{chunk['file']}-{i}.txt\"\n",
    "    with open(chunk_path, \"w\") as file:\n",
    "        file.write(chunk[\"chunk\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510f8c7-8cd3-4867-8742-9f4f9cda9e9f",
   "metadata": {},
   "source": [
    "## Authoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86c48e52-cda7-48ac-84dc-0b844aed5f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq docling-sdg\n",
    "\n",
    "# TODO: replace with above after https://github.com/docling-project/docling-sdg/pull/31 merges\n",
    "#!pip install -qq git+https://github.com/anastasds/docling-sdg@d15de2c5a81bfe166f66f412fc4b23728065f396"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a165c38-843b-4c89-a8ad-6195b998e284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking and filtering document safebalance-clarity-statement\n",
      "Created dataset safebalance-clarity-statement with 6 QA chunks\n"
     ]
    }
   ],
   "source": [
    "from docling_sdg.qa.utils import get_qa_chunks\n",
    "\n",
    "filters = [\n",
    "    lambda chunk: len(str(chunk.text)) > 500\n",
    "]\n",
    "\n",
    "dataset = {}\n",
    "for doc in docs:\n",
    "    print(f\"Chunking and filtering document {doc.document.name}\")\n",
    "\n",
    "    chunks = list(chunker.chunk(dl_doc=doc.document))\n",
    "    qa_chunks = list(get_qa_chunks(doc.document.name, chunk_objs, filters)) #TODO: decouple reference to chunk_objs from above)\n",
    "    dataset[doc.document.name] = qa_chunks\n",
    "    \n",
    "    print(f\"Created dataset {doc.document.name} with {len(qa_chunks)} QA chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65ec755-e3de-40ab-bf3a-23ebb29a705d",
   "metadata": {},
   "source": [
    "### Initialize QA generator, supplying details for which model to use\n",
    "\n",
    "GenerateOptions controls which model is used for QA generation by setting generate_options.provider below. Three options are available:\n",
    "\n",
    "* LlmProviders.WATSONX for watsonx\n",
    "* LlmProviders.OPENAI for OpenAI\n",
    "* LlmProviders.OPENAI_LIKE for any model provider with OpenAI compatible APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "874d4de8",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "API_KEY = \"a695b745e4bb13d054755b121894fa43\"  # the API access key for your account ( cannot be empty )\n",
    "API_URL = \"https://mixtral-8x7b-instruct-v0-1-maas-apicast-production.apps.prod.rhoai.rh-aiservices-bu.com:443/v1\"  # the URL of your model's API\n",
    "MODEL_ID = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # the name of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b702267e-f550-4bc2-bce4-c0fcecbbd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling_sdg.qa.generate import Generator\n",
    "from docling_sdg.qa.base import GenerateOptions, LlmProvider\n",
    "from pydantic import SecretStr\n",
    "\n",
    "generate_options = GenerateOptions(project_id=\"project_id\")\n",
    "generate_options.provider = LlmProvider.OPENAI_LIKE\n",
    "generate_options.api_key = SecretStr(API_KEY)\n",
    "generate_options.url = API_URL\n",
    "generate_options.model_id = MODEL_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919199c0-3747-409a-85ab-0155ef3ebe9d",
   "metadata": {},
   "source": [
    "### Configure subset selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1197d4e-8354-45e3-9ec9-85c78ba36548",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CHUNKS_TO_SELECT_FOR_AUTHORING = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2421d07-3e6c-4355-95f4-da8e157557c7",
   "metadata": {},
   "source": [
    "### Run QA generation on selected chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e57edff5-9a13-47fb-9248-9140ae5baaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing chunks that looks like:\n",
      "We don't charge overdraft fees on this account. To help you avoid overdrawing your account, transactions will be declined and returned unpaid when you don't have enough money in your account.\n",
      "¬∑ If this happens, you won't be charged a bank overdraft fee, but the merchant or third party could charge you a fee. For example, you may be charged a late fee if the payment isn't received on time.\n",
      "¬∑ An owner can set up email or mobile alerts to notify you when you have a low balance in your account or when items aren't paid. (We don't charge for this service but your mobile carrier's message and data fees may apply. Delivery of alerts may be affected or delayed by your mobile carrier's coverage.)\n",
      "While this account prevents you from overdrawing in most cases, there may still be times when your account could have a negative balance, but we won't charge an overdraft fee. This could happen in the following ways:\n",
      "Selected 5 contexts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:16<00:00,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "safebalance-clarity-statement: Status.SUCCESS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random #TODO: replace random sampling with subset selection\n",
    "\n",
    "for doc, chunks in dataset.items(): # TODO: multiple file support\n",
    "    generate_options.generated_file = AUTHORING_OUTPUT_DIR / f\"qagen-{doc}.json\"\n",
    "    gen = Generator(generate_options=generate_options)\n",
    "    print(f\"processing chunks that looks like:\\n{chunks[0].text}\")\n",
    "    selected_chunks = random.sample(chunks, NUM_CHUNKS_TO_SELECT_FOR_AUTHORING)\n",
    "    print(f\"Selected {len(selected_chunks)} contexts\")\n",
    "\n",
    "    Path.unlink(generate_options.generated_file, missing_ok=True)\n",
    "    results = gen.generate_from_chunks(selected_chunks) # automatically saves to file\n",
    "\n",
    "    print(f\"{doc}: {results.status}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2513fbb180dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea64b8f0-dd6c-4776-8646-9731433f909b",
   "metadata": {},
   "source": [
    "### Read generated QAs and restructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9df2c533-30d7-4c30-9907-7c5655fd2246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated QA pairs for 5 contexts\n",
      "[{'question': 'What is the usual availability of mobile check deposits?', 'answer': 'Usually the next business day if deposited by applicable cutoff times.'}, {'question': 'What are the typical waiting periods for various types of deposits?', 'answer': 'Cash, direct deposits and wire transfers are available on the day received, checks deposited in person or at ATMs are usually available the next business day if deposited before the cutoff time, and mobile check deposits are usually available the next business day if deposited by applicable cutoff times.'}, {'question': 'Under what circumstances might a deposit be held for a longer period?', 'answer': 'Deposits greater than $5,525 and checks deposited within the first 30 days of account opening may be held longer.'}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "from textwrap import wrap\n",
    "\n",
    "qnas = {}\n",
    "chunk_id_to_text = {}\n",
    "with open(generate_options.generated_file, \"rt\") as f:\n",
    "    for line in f.readlines():\n",
    "        entry = json.loads(line)\n",
    "        chunk_id = entry['chunk_id']\n",
    "        if chunk_id not in chunk_id_to_text:\n",
    "            chunk_id_to_text[chunk_id] = entry['context']\n",
    "        if chunk_id not in qnas:\n",
    "            qnas[chunk_id] = []\n",
    "        qnas[chunk_id].append({'question': entry['question'], 'answer': entry['answer']})\n",
    "\n",
    "print(f\"Generated QA pairs for {len(qnas)} contexts\")\n",
    "print(list(qnas.values())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6d6c26-f4d5-420d-ae78-ac28cf39efd3",
   "metadata": {},
   "source": [
    "### Define metadata for qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c7130e90-2b65-4008-86f7-194da74a9523",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCUMENT_OUTLINE = \"A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using Particle-Based Monte Carlo Methods\"\n",
    "DOMAIN = \"artificial intelligence\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dafa8927-e56c-448b-b88b-f8d854c25d4d",
   "metadata": {},
   "source": [
    "### Output qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d7f26460-737f-4940-b58a-ef6caea313d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qna.yaml saved to: workspaces/default/authoring/qna.yaml\n"
     ]
    }
   ],
   "source": [
    "qna_output_path = AUTHORING_OUTPUT_DIR / \"qna.yaml\"\n",
    "\n",
    "# The following creates a data structure for outputting in the expected format for qna.yaml\n",
    "# TODO: extract into utils library\n",
    "\n",
    "def str_presenter(dumper, data):\n",
    "  if len(data.splitlines()) > 1:  # check for multiline string\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  elif len(data) > 80:\n",
    "    data = \"\\n\".join(wrap(data, 80))\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "  return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "# to use with safe_dump:\n",
    "yaml.representer.SafeRepresenter.add_representer(str, str_presenter)\n",
    "\n",
    "class IndentedDumper(yaml.Dumper):\n",
    "    def increase_indent(self, flow=False, indentless=False):\n",
    "        return super(IndentedDumper, self).increase_indent(flow, False)\n",
    "\n",
    "data = {'seed_examples': []}\n",
    "for chunk_id, context in chunk_id_to_text.items():\n",
    "    data['seed_examples'].append({\n",
    "        'context': context,\n",
    "        'questions_and_answers': [\n",
    "            {\n",
    "                'question': example['question'],\n",
    "                'answer': example['answer'],\n",
    "            } for example in qnas[chunk_id]\n",
    "        ]\n",
    "    })\n",
    "\n",
    "data['document_outline'] = DOCUMENT_OUTLINE\n",
    "data['domain'] = DOMAIN\n",
    "\n",
    "Path.unlink(qna_output_path, missing_ok=True) # shouldn't be necessary but was. jupyter caching thing?\n",
    "with open(qna_output_path, 'w') as yaml_file:\n",
    "    yaml.dump(data, yaml_file, Dumper=IndentedDumper, default_flow_style=False, sort_keys=False, width=80)\n",
    "\n",
    "print(f\"qna.yaml saved to: {qna_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ea149-844b-4330-90ec-d0ca7ab12b90",
   "metadata": {},
   "source": [
    "### View generated qna.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1293d445-b826-4b92-ad20-9b121ac60e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed_examples:\n",
      "  - context: \"\\xB7 Cash, direct deposits and wire transfers: On the day we receive\\\n",
      "      \\ them. 4\\n\\xB7 Checks you deposit: Usually the next business day, if deposited\\\n",
      "      \\ before the financial center or ATM cutoff time.\\n\\xB7 Mobile Check Deposits:\\\n",
      "      \\ Usually the next business day if deposited by applicable cutoff times. Please\\\n",
      "      \\ refer to Deposit Checks , then Help in the Mobile Banking app for additional\\\n",
      "      \\ details and terms and conditions.\\n\\xB7 If we place a hold on your deposit,\\\n",
      "      \\ we'll let you know the hold reason and when funds will be available for you\\\n",
      "      \\ to use. This is typically provided at the time of deposit but may also be\\\n",
      "      \\ mailed later. Deposits greater than $5,525 and checks deposited within the\\\n",
      "      \\ first 30 days of account opening may be held longer.\"\n",
      "    questions_and_answers:\n",
      "      - question: What is the usual availability of mobile check deposits?\n",
      "        answer: Usually the next business day if deposited by applicable cutoff times.\n",
      "      - question: What are the typical waiting periods for various types of deposits?\n",
      "        answer: |-\n",
      "          Cash, direct deposits and wire transfers are available on the day received,\n",
      "          checks deposited in person or at ATMs are usually available the next business\n",
      "          day if deposited before the cutoff time, and mobile check deposits are usually\n",
      "          available the next business day if deposited by applicable cutoff times.\n",
      "      - question: Under what circumstances might a deposit be held for a longer period?\n",
      "        answer: |-\n",
      "          Deposits greater than $5,525 and checks deposited within the first 30 days of\n",
      "          account opening may be held longer.\n",
      "  - context: \"The way we post transactions impacts your account balance. At the end\\\n",
      "      \\ of each business day, we'll group transactions received that day into categories\\\n",
      "      \\ before posting them. Below are the most common categories, and common transaction\\\n",
      "      \\ types in each, in the order that they generally post to your account. Keep\\\n",
      "      \\ in mind that transactions that are still processing may lower your available\\\n",
      "      \\ balance.\\n\\xB7 Deposits: Added from highest to lowest dollar amount.\\n\\xB7\\\n",
      "      \\ Many debit transactions: Subtracted based on the date and time you made them.\\\n",
      "      \\ If our system doesn't receive date and time of the transaction, they are posted\\\n",
      "      \\ and subtracted from highest to lowest dollar amount. These include one-time\\\n",
      "      \\ and recurring debit card transactions, one-time transfers and ATM withdrawals.\\n\\\n",
      "      \\xB7 Most other electronic payments and preauthorized transfers: Subtracted\\\n",
      "      \\ from highest to lowest dollar amount. These include scheduled transfers, online\\\n",
      "      \\ bill payments and preauthorized payments that use your account number.\\n\\u2022\\\n",
      "      \\ Most fees:\\nSubtracted from highest to lowest dollar amounts.\\nReminder: Paper\\\n",
      "      \\ checks will not work with this account.\"\n",
      "    questions_and_answers:\n",
      "      - question: |-\n",
      "          What type of transactions are subtracted from the account balance based on the\n",
      "          date and time they were made?\n",
      "        answer: Many debit transactions\n",
      "      - question: |-\n",
      "          What are the general steps in which transactions are posted to a customer's\n",
      "          account at the end of a business day?\n",
      "        answer: |-\n",
      "          Transactions are grouped into categories, with deposits added from highest to\n",
      "          lowest dollar amount, many debit transactions subtracted based on the date and\n",
      "          time they were made or their dollar amount if the date and time are not\n",
      "          available, most other electronic payments and preauthorized transfers subtracted\n",
      "          from highest to lowest dollar amount, and most fees subtracted from highest to\n",
      "          lowest dollar amounts.\n",
      "      - question: |-\n",
      "          Based on the provided information, why would paper checks not work with this\n",
      "          account?\n",
      "        answer: |-\n",
      "          The text does not provide a specific reason, but it might be due to the\n",
      "          account's electronic-focused transaction handling.\n",
      "  - context: \"Review all the features and benefits of your new account at\\nbankofamerica.com/quickstart\\n\\\n",
      "      For questions, schedule an appointment\\nto visit a financial center at\\nbankofamerica.com/appointments\\n\\\n",
      "      Call us at 800.432.1000\\nPlease see the Personal Schedule of Fees and Deposit\\\n",
      "      \\ Agreement and Disclosures for your account terms.\\n1. Some payment options\\\n",
      "      \\ are unavailable for the owner and child of a SafeBalance \\xAE  for Family\\\n",
      "      \\ Banking account.\\n2. Mobile Banking requires that you download the Mobile\\\n",
      "      \\ Banking app and is only available for select mobile devices. Message and data\\\n",
      "      \\ rates may apply.\\n3. Fees apply to wires and certain transfers. See the Online\\\n",
      "      \\ Banking Service Agreement at bankofamerica.com/serviceagreement for details.\\n\\\n",
      "      4. Direct deposit and wire transfers aren't available for SafeBalance \\xAE \\\n",
      "      \\ for Family Banking accounts.\\n5. SafeBalance \\xAE  for Family Banking accounts\\\n",
      "      \\ can't receive incoming wire transfers.\"\n",
      "    questions_and_answers:\n",
      "      - question: \"What banking feature is not available for SafeBalance \\xAE  for\\\n",
      "          \\ Family Banking\\naccounts?\"\n",
      "        answer: \"Direct deposit and wire transfers aren't available for SafeBalance\\\n",
      "          \\ \\xAE  for Family\\nBanking accounts.\"\n",
      "      - question: \"What are some limitations of the SafeBalance \\xAE  for Family Banking\\\n",
      "          \\ account?\"\n",
      "        answer: \"SafeBalance \\xAE  for Family Banking accounts have several limitations,\\\n",
      "          \\ such as\\nunavailability of certain payment options for the account owner\\\n",
      "          \\ and child,\\ninability to receive incoming wire transfers, and unavailability\\\n",
      "          \\ of direct\\ndeposit and wire transfers.\"\n",
      "      - question: \"Based on the information, why do SafeBalance \\xAE  for Family Banking\\\n",
      "          \\ accounts seem\\nto be less flexible than other accounts?\"\n",
      "        answer: \"SafeBalance \\xAE  for Family Banking accounts seem to be less flexible\\\n",
      "          \\ than other\\naccounts because they have a limited range of banking features.\\\n",
      "          \\ For instance,\\nthey don't support direct deposit, wire transfers, or incoming\\\n",
      "          \\ wire transfers.\\nAdditionally, some payment options are unavailable for\\\n",
      "          \\ the account owner and\\nchild.\"\n",
      "  - context: |-\n",
      "      Card replacement, No fee = No fee. Card replacement, Paper copies available upon\n",
      "      request. Printable statements are available in Online Banking. = For each ATM or\n",
      "      debit card. Card replacement, No fee = $15.00. Card replacement, Paper copies\n",
      "      available upon request. Printable statements are available in Online Banking. =\n",
      "      For rush delivery. Stop payment, No fee = $30.00. Stop payment, Paper copies\n",
      "      available upon request. Printable statements are available in Online Banking. =\n",
      "      For each request; WAIVED for requests on debit card or Bill Pay transactions.\n",
      "      Cashier's checks, No fee = $15.00. Cashier's checks, Paper copies available upon\n",
      "      request. Printable statements are available in Online Banking. = For each check.\n",
      "      Domestic wire transfers, No fee = $15.00. Domestic wire transfers, Paper copies\n",
      "      available upon request. Printable statements are available in Online Banking. =\n",
      "      For each incoming wire transfer 5. Domestic wire transfers, No fee = $30.00.\n",
      "      Domestic wire transfers, Paper copies available upon request. Printable\n",
      "      statements are available in Online Banking. = For each outgoing wire transfer.\n",
      "      International wire transfers, No fee = $15.00. International wire transfers,\n",
      "      Paper copies available upon\n",
      "    questions_and_answers:\n",
      "      - question: What is the fee for a stop payment?\n",
      "        answer: $30.00\n",
      "      - question: What services are available for free in Online Banking?\n",
      "        answer: Printable statements are available for free in Online Banking.\n",
      "      - question: |-\n",
      "          If you request a rush delivery for a card replacement, what fee will you be\n",
      "          charged?\n",
      "        answer: $15.00\n",
      "  - context: \"We don't charge overdraft fees on this account. To help you avoid overdrawing\\\n",
      "      \\ your account, transactions will be declined and returned unpaid when you don't\\\n",
      "      \\ have enough money in your account.\\n\\xB7 If this happens, you won't be charged\\\n",
      "      \\ a bank overdraft fee, but the merchant or third party could charge you a fee.\\\n",
      "      \\ For example, you may be charged a late fee if the payment isn't received on\\\n",
      "      \\ time.\\n\\xB7 An owner can set up email or mobile alerts to notify you when\\\n",
      "      \\ you have a low balance in your account or when items aren't paid. (We don't\\\n",
      "      \\ charge for this service but your mobile carrier's message and data fees may\\\n",
      "      \\ apply. Delivery of alerts may be affected or delayed by your mobile carrier's\\\n",
      "      \\ coverage.)\\nWhile this account prevents you from overdrawing in most cases,\\\n",
      "      \\ there may still be times when your account could have a negative balance,\\\n",
      "      \\ but we won't charge an overdraft fee. This could happen in the following ways:\"\n",
      "    questions_and_answers:\n",
      "      - question: Does this account charge overdraft fees?\n",
      "        answer: No, this account does not charge overdraft fees.\n",
      "      - question: |-\n",
      "          What are the possible consequences if a transaction is declined due to\n",
      "          insufficient funds in this account?\n",
      "        answer: |-\n",
      "          The merchant or third party could charge a fee, such as a late fee if the\n",
      "          payment isn't received on time.\n",
      "      - question: |-\n",
      "          If your account has a negative balance but no overdraft fee is charged, why\n",
      "          might this occur?\n",
      "        answer: |-\n",
      "          This could occur due to internal processes or systems that allow for a negative\n",
      "          balance, but the bank has chosen not to charge an overdraft fee in these\n",
      "          instances.\n",
      "document_outline: |-\n",
      "  A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using\n",
      "  Particle-Based Monte Carlo Methods\n",
      "domain: artificial intelligence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(qna_output_path) as yaml_file:\n",
    "    print(yaml_file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c574f96-5860-48b9-b4ac-01d367c7717b",
   "metadata": {},
   "source": [
    "### Revise QAs\n",
    "\n",
    "Open the generated `qna.yaml` in your preferred text editor to ensure the quality of generated questions and answers. If the generation step has failed to generated three questions and answers for each of five contexts, supplant until that required number of QA pairs is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f101076-a50f-49ea-a83b-46eaa8b39cc4",
   "metadata": {},
   "source": [
    "## Create Seed Dataset for SDG\n",
    "\n",
    "This notebook combines the contents from the qna.yaml and the chunks from the source document to create a seed dataset for the synthetic data generation process.\n",
    "\n",
    "To run this notebook you need a directory that contains N chunks named `{original-file-name}-{N}.txt` and a `qna.yaml` in the same directory.\n",
    "\n",
    "This notebook outputs a `seed.jsonl` file in the `output_dir` that you set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2c6e31b-e8a9-406c-b2dc-27433c8fd8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab2c9ed2-8ba8-4959-8e01-81625b81d286",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 4334.08 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 4756.35 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 4989.75 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 5548.63 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12/12 [00:00<00:00, 5405.61 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:00<00:00, 3062.73 examples/s]\n",
      "Filter: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:00<00:00, 3761.31 examples/s]\n",
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 608.05ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 60 rows\n",
      "Results saved to: workspaces/default/seed_examples/seed_data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "from utils.create_seed_dataset import get_seed_dataset\n",
    "\n",
    "src_files = os.listdir(CHUNKING_OUTPUT_DIR)\n",
    "\n",
    "for file_name in src_files:\n",
    "    full_file_name = os.path.join(CHUNKING_OUTPUT_DIR, file_name)\n",
    "    if os.path.isfile(full_file_name):\n",
    "        shutil.copy(full_file_name, SEED_EXAMPLE_INPUT_DIR)\n",
    "\n",
    "shutil.copy(qna_output_path, SEED_EXAMPLE_INPUT_DIR)\n",
    "\n",
    "seed_data = get_seed_dataset(SEED_EXAMPLE_INPUT_DIR)\n",
    "output_path = f'{SEED_EXAMPLE_OUTPUT_DIR}/seed_data.jsonl'\n",
    "seed_data.to_json(output_path, orient='records', lines=True)\n",
    "\n",
    "print(f\"Generated {seed_data.data.num_rows} rows\")\n",
    "print(f\"Results saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff36f4-19fc-4a27-b51a-3688e7b630e4",
   "metadata": {},
   "source": [
    "### Inspect the generated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6936825-31c1-4b46-a1af-2fb46f50158d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pyarrow.Table\n",
      "document_outline: string\n",
      "document_title: string\n",
      "domain: string\n",
      "icl_document: string\n",
      "icl_query_1: string\n",
      "icl_response_1: string\n",
      "icl_query_2: string\n",
      "icl_response_2: string\n",
      "icl_query_3: string\n",
      "icl_response_3: string\n",
      "document: string\n",
      "----\n",
      "document_outline: [[\"A Probabilistic Inference Approach to Inference-Time Scaling of LLMs using\n",
      "Particle-Based Monte Carlo Methods\"]]\n",
      "document_title: [[\"safebalance-clarity-statement\"]]\n",
      "domain: [[\"artificial intelligence\"]]\n",
      "icl_document: [[\"¬∑ Cash, direct deposits and wire transfers: On the day we receive them. 4\n",
      "¬∑ Checks you deposit: Usually the next business day, if deposited before the financial center or ATM cutoff time.\n",
      "¬∑ Mobile Check Deposits: Usually the next business day if deposited by applicable cutoff times. Please refer to Deposit Checks , then Help in the Mobile Banking app for additional details and terms and conditions.\n",
      "¬∑ If we place a hold on your deposit, we'll let you know the hold reason and when funds will be available for you to use. This is typically provided at the time of deposit but may also be mailed later. Deposits greater than $5,525 and checks deposited within the first 30 days of account opening may be held longer.\"]]\n",
      "icl_query_1: [[\"What is the usual availability of mobile check deposits?\"]]\n",
      "icl_response_1: [[\"Usually the next business day if deposited by applicable cutoff times.\"]]\n",
      "icl_query_2: [[\"What are the typical waiting periods for various types of deposits?\"]]\n",
      "icl_response_2: [[\"Cash, direct deposits and wire transfers are available on the day received,\n",
      "checks deposited in person or at ATMs are usually available the next business\n",
      "day if deposited before the cutoff time, and mobile check deposits are usually\n",
      "available the next business day if deposited by applicable cutoff times.\"]]\n",
      "icl_query_3: [[\"Under what circumstances might a deposit be held for a longer period?\"]]\n",
      "icl_response_3: [[\"Deposits greater than $5,525 and checks deposited within the first 30 days of\n",
      "account opening may be held longer.\"]]\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(seed_data.data.table.slice(length=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a8fcdb-8035-4f30-b856-46afe9f928a1",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "To recap, given a source document in PDF format, this notebook:\n",
    "\n",
    "1. Converted the document using document and saved it to JSON for inspection\n",
    "2. Split the extracted text into chunks\n",
    "3. Generated QA pairs for a subset of those chunks\n",
    "4. Created a `qna.yaml` available for inspection and revision\n",
    "5. Combined the chunks and `qna.yaml` to create a `seed.jsonl` for use with SDG\n",
    "\n",
    "The next step is to use the resulting `seed.jsonl` for SDG, such as illustrated in [this notebook](https://github.com/Red-Hat-AI-Innovation-Team/sdg_hub/blob/main/examples/instructlab/knowledge/knowledge_generation_and_mixing.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd834cc-f037-49e4-b2c8-256ebba2b802",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
